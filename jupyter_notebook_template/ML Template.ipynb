{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Setup](#section1)<br>\n",
    "    1.1 [Installing Commands](#section11)<br>\n",
    "    1.2 [Importing Libraries](#section12)<br>\n",
    "    1.3 [OS package commands](#section13)<br>\n",
    "    1.4 [Data Acquisition](#section14)<br>\n",
    "2. [Pre-Processing](#section2)<br>\n",
    "3. [Modeling](#section3)<br>\n",
    "4. [Model Evaluation](#section4)<br>\n",
    "5. [ML Pipeline](#section5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section1></a> \n",
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section11></a> \n",
    "## Install a package command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q <package_name>\n",
    "# !pip install -q datascience\n",
    "# !pip install -q pandas-profiling\n",
    "# !pip install -q yellowbrick\n",
    "# -q means quiet install\n",
    "# !pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section12></a> \n",
    "## Import Packages commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to IPython!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Your favorite libraries have been loaded.\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis packages\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 500) # OR pd.options.display.max_rows = 500\n",
    "pd.set_option('display.max_columns', 500) # OR pd.options.display.max_columns = 500\n",
    "pd.options.display.float_format = \"{:,.2f}\".format # to avoid seeing exponential data in describe output\n",
    "import pandas_profiling \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"font\", size=14)\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# Other useful packages\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from subprocess import check_output\n",
    "from pydotplus.graphviz import graph_from_dot_data\n",
    "from graphviz import Source\n",
    "\n",
    "# Sklearn API\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Sklearn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Sklearn Preprocesing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#Importing Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#Importing Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "# import lightgbm as lgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "\n",
    "#Importing Unsupervised Models\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Classification Algo Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "\n",
    "# Regression Algo Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "#Pipeline with Pandas\n",
    "import pdpipe as pdp\n",
    "\n",
    "# Stats API\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# If in ipython, load autoreload extension\n",
    "if 'ipython' in globals():\n",
    "    print('\\nWelcome to IPython!')\n",
    "    ipython.magic('load_ext autoreload')\n",
    "    ipython.magic('autoreload 2')\n",
    "\n",
    "# Display all cell outputs in notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "print('Your favorite libraries have been loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section13></a> \n",
    "## OS package commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'ExpediaGroupDataScienceAcademy', 'gitrepos', 'UdemyCourses', '.DS_Store', 'PythonandS3', 'Tableau', 'NaturalLanguageProcessing', 'Computer Vision', 'Python', 'Data Visualization', 'R', 'Fee Receipt', 'MachineLearning', 'Numpy', 'EDA Project', 'PythonandStatistics', 'PyCharmProjects', 'INSAIDGCDProgramSyllabus', 'YoutubeVideos', 'AboutDataScience', 'Useful EDA commands.ipynb', 'WebScrappingScript', 'PythonForFinance', 'PythonDSPresentations', '.ipynb_checkpoints', 'PythonPractice', 'Deep Learning', 'Assignments - Numpy and Pandas', 'ODSCMeetup', 'Data', 'PythonandSparkforBigData', 'Pandas', 'AnalyticsLab', 'Career Guide']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/Users/rajkgupta/DATASCIENCE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(), 'data', 'titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section14></a> \n",
    "## Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-2/master/CaseStudy/Advertising.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filepath_or_buffer='https://storage.googleapis.com/industryanalytics/trans_fraud_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Google Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "diabetes = pd.read_csv('/content/drive/My Drive/TensorFlow/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section2></a> \n",
    "# 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.describe()\n",
    "df.info()\n",
    "df.shape\n",
    "display(df.head())\n",
    "df['column_name'].mode()\n",
    "df = df.drop(['column_name'], axis = 1)\n",
    "df.drop(['column_name'], axis = 1, inplace=True)\n",
    "median_column = df.column_name.median()\n",
    "df.column_name = df.column_name.fillna(df['column_name'].mode()[0])\n",
    "df['Age'].fillna(df['Age'].dropna().median(), inplace=True)\n",
    "df.column_name = df.apply(lambda x: 'child' if x['Age'] < 15 else x['Sex'],axis=1)\n",
    "df = pd.get_dummies(df, columns=['col1','col2'], drop_first=True)\n",
    "df = pd.concat([df1, df2],axis=1)\n",
    "df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} \n",
    "df['Title'] = df['Title'].map(title_mapping)\n",
    "data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col'], 'Rare')\n",
    "# Categorizing Numerical Value \n",
    "data['FareBand'] = pd.qcut(data['Fare'], 4).astype(str)\n",
    "dataDump  = df.copy()\n",
    "df.skew()\n",
    "\n",
    "# Combining the categories inside single column\n",
    "def combine(x):\n",
    "    if x in ['FV', 'RH']: \n",
    "        return 'FRC'\n",
    "    elif 'C' in x: \n",
    "        return 'FRC'\n",
    "    else:\n",
    "        return x\n",
    "prices['MSZoning'] = prices['MSZoning'].apply(lambda x : combine(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data audit Report for continuous variables\n",
    "def continuous_var_summary(x):\n",
    "    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  \n",
    "                      x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),\n",
    "                          x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), \n",
    "                              x.quantile(0.90),x.quantile(0.95), x.quantile(0.99),x.max()], \n",
    "                  index = ['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1', \n",
    "                               'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data audit Report for categorical variables\n",
    "def categorical_var_summary(x):\n",
    "    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n",
    "    return pd.Series([x.count(), x.isnull().sum(), Mode.iloc[0, 0], Mode.iloc[0, 1], \n",
    "                          round(Mode.iloc[0, 1] * 100/x.count(), 2)], \n",
    "                  index = ['N', 'NMISS', 'MODE', 'FREQ', 'PERCENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation for categorical and continuous variables\n",
    "def missing_imputation(x, stats = 'mean'):\n",
    "    if (x.dtypes == 'float64') | (x.dtypes == 'int64'):\n",
    "        x = x.fillna(x.mean()) if stats == 'mean' else x.fillna(x.median())\n",
    "    else:\n",
    "        x = x.fillna(x.mode())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An utility function to create dummy variable\n",
    "def create_dummies(df, colname):\n",
    "    col_dummies = pd.get_dummies(df[colname], prefix = colname, drop_first = True)\n",
    "    df = pd.concat([df, col_dummies], axis = 1)\n",
    "    df.drop(colname, axis = 1, inplace = True )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Pandas column into multiple columns using Get Dummies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_name].str.get_dummies(sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_name].str.split(\";\",expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Combine and Separate Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Id = df_test.Id\n",
    "train_Id = df_train.Id\n",
    "df_train.drop(['Id'], axis=1, inplace=True)\n",
    "df_test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "train_index = df_train.shape[0]\n",
    "test_index = df_test.shape[0]\n",
    "train_Target = df_train.Target\n",
    "all_data = pd.concat((df_train, df_test)).reset_index(drop=True)\n",
    "all_data.drop([target_col], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))\n",
    "\n",
    "\n",
    "df_train = all_data[:train_index]\n",
    "df_test = all_data[train_index:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_Id\n",
    "sub['SalePrice'] = test_SalePrice\n",
    "sub.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sepearate categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate categorical and continuous variables\n",
    "df_cont = df.loc[:, (df.dtypes == 'float64') | (df.dtypes == 'int64')]\n",
    "df_cat = df.loc[:, (df.dtypes == 'object')]\n",
    "\n",
    "# Simper way of doing:\n",
    "df_cont = df.select_dtypes(include = ['float64', 'int64'])\n",
    "df_cat = df.select_dtypes(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars.apply(continuous_var_summary).T.round(1)\n",
    "cars_cat_vars.apply(categorical_var_summary).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col] = df[col].astype('category')\n",
    "df[col] = df[col].astype('int64')\n",
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. category\n",
    "2. object\n",
    "3. int64\n",
    "4. float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_frame = pd.DataFrame(data.isnull().sum(), columns = ['Frequency'])\n",
    "null_frame.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rownum = 59381.0\n",
    "desc = df.describe().T\n",
    "desc[desc['count']!=rownum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars = cars_conti_vars.apply(missing_imputation)\n",
    "cars_cat_vars = cars_cat_vars.apply(missing_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_na = (insurance_df.isnull().sum() / len(insurance_df)) * 100\n",
    "dataset_na = dataset_na.drop(dataset_na[dataset_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :dataset_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 90\n",
    "prices_missing_threshold = (prices.isnull().sum() / len(prices)) * 100\n",
    "for col in prices.columns:\n",
    "    if prices_missing_threshold.loc[col]>thrshold_na:\n",
    "        prices.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_frame = pd.DataFrame(index = data.columns.values)\n",
    "null_frame['Null Frequency'] = data.isnull().sum().values\n",
    "percent = data.isnull().sum().values/data.shape[0]\n",
    "null_frame['Missing %age'] = np.round(percent, decimals = 4) * 100\n",
    "null_frame.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "\n",
    "https://github.com/awslabs/datawig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas.fillna\n",
    "\n",
    "Sklearn.impute\n",
    "\n",
    "AWS Datawig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#backward\n",
    "\n",
    "https://contrib.scikit-learn.org/categorical-encoding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backward Difference Coding\n",
    "* hashing\n",
    "* leave one out\n",
    "* one hot\n",
    "* ordinal\n",
    "* polynoimial coding\n",
    "* target encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification & Handling of Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Contains Duplicate Rows?', data.duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(df.corr(),cmap='Blues',annot=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_corr_features(df, threshold):\n",
    "    corr_features = set()\n",
    "    # create the correlation matrix (default to pearson)\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix .columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colnamei = corr_matrix.columns[i]\n",
    "                colnamej = corr_matrix.columns[j]\n",
    "                corr_features.add( (colnamei,colnamej) )\n",
    "                # corr_features.append(colnamej)\n",
    "    return corr_features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction/Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Addition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.columns.values\n",
    "number_of_columns=12\n",
    "number_of_rows = len(l)-1/number_of_columns\n",
    "plt.figure(figsize=(number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(l)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.boxplot(df[l[i]],color='green',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=[]\n",
    "def detect_outlier(data_1):\n",
    "    \n",
    "    threshold=3\n",
    "    mean_1 = np.mean(data_1)\n",
    "    std_1 =np.std(data_1)\n",
    "    \n",
    "    \n",
    "    for y in data_1:\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(y)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars = cars_conti_vars.apply(lambda x: x.clip(lower = x.dropna().quantile(0.01), \n",
    "                                                         upper = x.quantile(0.99)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check individual Column distribution (Plotly Express) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( data.corr(), annot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality correlation matrix\n",
    "k = 12 #number of variables for heatmap\n",
    "cols = df.corr().nlargest(k, 'quality')['quality'].index\n",
    "cm = df[cols].corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = titanic_data.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr,vmax=.8,linewidth=.01, square = True, annot = True,cmap='YlGnBu',linecolor ='black')\n",
    "plt.title('Correlation between features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = [12, 8])\n",
    "sns.heatmap(data_transfer.corr(), annot = True, cmap = 'YlGnBu')\n",
    "plt.title('Correlation between Features', size = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, size = 2, aspect = 1.5)\n",
    "sns.pairplot(glass, hue=\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, x_vars=['TV', 'radio', 'newspaper'], y_vars='sales', size=5, aspect=1, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(titanic_data[[\"Fare\",\"Age\",\"Pclass\",\"Survived\"]],vars = [\"Fare\",\"Age\",\"Pclass\"],hue=\"Survived\", dropna=True,markers=[\"o\", \"s\"])\n",
    "plt.title('Pair Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "g = sns.pairplot(iris,hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_labels = review_data_k_means[['labels','Age']]\n",
    "age_labels.boxplot(by='labels',figsize=(20,10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(glass, hue=\"Type\", size=5) \\\n",
    "   .map(plt.scatter, \"Fe\", \"Ba\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)                                      # Set up the matplotlib figure\n",
    "sns.despine(left=True)\n",
    "sns.distplot(data.sales, color=\"b\", ax=axes[0, 0])\n",
    "sns.distplot(data.TV, color=\"r\", ax=axes[0, 1])\n",
    "sns.distplot(data.radio, color=\"g\", ax=axes[1, 0])\n",
    "sns.distplot(data.newspaper, color=\"m\", ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.title('Distribution of Each Column in the Data')\n",
    "for i,col in enumerate(review_data_k_means.columns):\n",
    "    plt.figure(i)\n",
    "    sns.distplot(review_data_k_means[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JG1 = sns.jointplot(\"newspaper\", \"sales\", data=data, kind='reg')\n",
    "JG2 = sns.jointplot(\"radio\", \"sales\", data=data, kind='reg')\n",
    "JG3 = sns.jointplot(\"TV\", \"sales\", data=data, kind='reg')\n",
    "#subplots migration\n",
    "f = plt.figure()\n",
    "for J in [JG1, JG2,JG3]:\n",
    "    for A in J.fig.axes:\n",
    "        f._axstack.add(f._make_key(A), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(l)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.distplot(df[l[i]],kde=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Andrews Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas.tools\n",
    "from pandas.plotting import andrews_curves\n",
    "andrews_curves(glass[glass['Type'].isin([2,3,4,5,6])], \"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name']=np.log(df['col_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling of Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(data)\n",
    "data1 = scaler.transform(data)\n",
    "data = pd.DataFrame(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "sent_scaled = min_max_scaler.fit_transform(review_data_k_means[['Sentiment']].values)\n",
    "#df_normalized = pd.DataFrame(x_scaled)\n",
    "sent_scaled\n",
    "review_data_k_means['Sentiment_Norm'] = sent_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the Imbalance in Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 33)\n",
    "X_train_new, y_train_new = sm.fit_sample(X_train, y_train.ravel())\n",
    "pd.Series(y_train_new).value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Apply for showing progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook().pandas()\n",
    "movies.progress_apply(lambda x: calculcateNewRating(x['Genre'],x['Rating']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd7428f262e4fee98a55b2c622047c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='variables', max=9.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d90ac903c41edb5609e06dae19851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='correlations', max=6.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6527dfc2e441c19dbebeb95aa2458d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='interactions [continuous]', max=64.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31633cc8e1bc461a819f0cbdb1ef9e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='table', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f750ab79af4ddfbc1c5bfd78add01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='missing', max=2.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca0008eba84f50ab81aa71c6c8f6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='warnings', max=3.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ca486ac9964d13bf46071566588b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='package', max=1.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb55a28782d34f479736312f8dbbd512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='build report structure', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv('/Users/rajkgupta/Downloads/diabetes.csv')\n",
    "report = pandas_profiling.ProfileReport(db)\n",
    "report.to_file(output_file = '/Users/rajkgupta/Downloads/pre_profilereport_diabetes.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section3></a> \n",
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classification Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach would be oversampling the minority class using SMOTE (Synthetic Minority Oversampling Technique) present in python library known as imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,df.columns != 'target_name']\n",
    "y = df['target_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "logreg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier(random_state = 0,criterion=\"entropy\")\n",
    "rf = RandomForestClassifier(random_state = 0)\n",
    "knn = KNeighborsClassifier(n_neighbors=13, p=2, metric='minkowski',n_jobs=-1) # Euclidean  # 3, 7, 13, 20 \n",
    "regressor = KNeighborsRegressor(n_neighbors=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting model of decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(export_graphviz(dt, out_file=None,filled=True, \n",
    "                                rounded=True,  \n",
    "                                special_characters=True, feature_names=X.columns))\n",
    "graph.format = 'png'\n",
    "graph.render('dtree_render',view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters , it should always be a dictionary \n",
    "parameters = {\n",
    "                'normalize':[True,False], \n",
    "                'copy_X':[True, False], \n",
    "                'fit_intercept':[True,False]\n",
    "             }\n",
    "linreg = LinearRegression() # this the model on which i would want to experiment \n",
    "# Call the GridSearch Class, Pass the model and parameter \n",
    "linreg = GridSearchCV(linreg,parameters)\n",
    "linreg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\n",
    "        \"C\":np.logspace(-3,3,7), \n",
    "        \"penalty\":[\"l1\",\"l2\"], # l1 lasso l2 ridge\n",
    "        \"tol\":[0.01,0.001,0.0001]  \n",
    "      } \n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_para = [\n",
    "            {\n",
    "                'criterion':['gini','entropy'],\n",
    "                'max_depth': range(2,60),\n",
    "                'max_features': ['sqrt', 'log2', None] \n",
    "            }\n",
    "            ]\n",
    "grid_search = GridSearchCV(dt,tree_para, cv=10, refit='AUC')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": range(2,20),\n",
    "              \"min_samples_split\": sp_randint(5, 25),\n",
    "              \"min_samples_leaf\": sp_randint(5, 20),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"n_estimators\": [20,50,100, 400, 700, 1000, 1500],\n",
    "              \"criterion\" : [\"gini\", \"entropy\"],\n",
    "              'max_features': ['sqrt', 'log2', None]\n",
    "             }\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(rf, param_distributions = param_dist,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   n_jobs = -1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)\n",
    "y_pred_test = linreg.predict([X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_train)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means Elbow Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "number_of_clusters = range(5,40)\n",
    "kmeans = [KMeans(n_clusters=i,max_iter=1000,random_state=42) for i in number_of_clusters]\n",
    "score = [-1*kmeans[i].fit(review_data_std).score(review_data_std) for i in range(len(kmeans))]\n",
    "pl.plot((number_of_clusters),score)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_test = KMeans(n_clusters=6,max_iter=5000,random_state=42)\n",
    "k_means_test.fit(review_data_std).score(review_data_std)\n",
    "k_means_test.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(eachRow):\n",
    "  TotalSum = []\n",
    "  for eachWord in eachRow.split():\n",
    "    if eachWord.lower() in d:   # if it is found in the dictionary \n",
    "      TotalSum.append(int(d[eachWord.lower()]))\n",
    "    else:\n",
    "      TotalSum.append(0)\n",
    "  return np.sum(TotalSum)\n",
    "  # return TotalSum\n",
    "\n",
    "\n",
    "review_data_k_means['Sentiment'] = review_data_k_means['Review Text'].apply(getSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def getSentimentTextBlob(eachRow):\n",
    "  return TextBlob(eachRow).sentiment.polarity\n",
    "\n",
    "review_data_k_means['Sentiment_textblob'] = review_data_k_means['Review Text'].apply(getSentimentTextBlob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section4></a> \n",
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "MAE_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "print('MAE for training set is {}'.format(MAE_train))\n",
    "print('MAE for test set is {}'.format(MAE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "MSE_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "print('MSE for training set is {}'.format(MSE_train))\n",
    "print('MSE for test set is {}'.format(MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt( metrics.mean_squared_error(y_train, y_pred_train))\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE for training set is {}'.format(RMSE_train))\n",
    "print('RMSE for test set is {}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean cross-validated score of the best_estimator : \", linreg.best_score_)  # In case of Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Coeficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept:',linreg.intercept_)                                           # print the intercept \n",
    "print('Coefficients:',linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols.insert(0,'Intercept')\n",
    "coef = linreg.coef_.tolist()\n",
    "coef.insert(0, linreg.intercept_)\n",
    "eq1 = zip(feature_cols, coef)\n",
    "\n",
    "for c1,c2 in eq1:\n",
    "    print(c1,c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = linreg.predict(X_train)\n",
    "SS_Residual = sum((y_train-yhat)**2)\n",
    "SS_Total = sum((y_train-np.mean(y_train))**2)\n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score for test data is:', accuracy_score(y_test,y_pred_test))\n",
    "\n",
    "# Using predict_proba\n",
    "preds1 = np.where(logreg.predict_proba(X_test)[:,1]> 0.75,1,0)\n",
    "print('Accuracy score for test data is:', accuracy_score(y_test,preds1))\n",
    "preds2 = np.where(logreg.predict_proba(X_test)[:,1]> 0.25,1,0)\n",
    "print('Accuracy score for test data is:', accuracy_score(y_test,preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test))\n",
    "confusion_matrix.index = ['Actual Died','Actual Survived']\n",
    "confusion_matrix.columns = ['Predicted Died','Predicted Survived']\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/Recall/F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision score for test data using model1 is:', precision_score(y_test,prediction1))\n",
    "print('Recall score for test data using model1 is:',recall_score(y_test,prediction1)) \n",
    "print('F1_score for test data using model1 is:',f1_score(y_test, prediction1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_report = classification_report(y_test, y_pred)\n",
    "print(random_forest_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-AUC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = model1.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRCurve(model):\n",
    "  '''\n",
    "  A function to compute Precision Recall Curve\n",
    "  Data to fit must be training i.e. X_train, y_train\n",
    "  Data score will be estimated on X_test, y_test\n",
    "  '''\n",
    "  viz = PrecisionRecallCurve(model)\n",
    "  viz.fit(X_train, y_train)\n",
    "  avg_prec = viz.score(X_test, y_test)\n",
    "  plt.legend(labels = ['Binary PR Curve',\"AP=%.3f\"%avg_prec], loc = 'lower right', prop={'size': 14})\n",
    "  plt.xlabel(xlabel = 'Recall', size = 14)\n",
    "  plt.ylabel(ylabel = 'Precision', size = 14)\n",
    "  plt.title(label = 'Precision Recall Curve', size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section5></a> \n",
    "# 5. ML Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = ['Survived']\n",
    "data1_x_bin = pd.get_dummies(df2)\n",
    "\n",
    "# X = data1_x_bin\n",
    "# y = Target\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .7, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time', 'TrainTestDifference']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data[Target]  # Y \n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "Feature_Importance = {}\n",
    "\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1_x_bin, data[Target], cv  = cv_split,return_train_score=True,scoring='precision')\n",
    "\n",
    "    # cv_result is a doctionary -> All the results of diff models are saved \n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    #MLA_compare.loc[row_index, 'TrainTestDifference'] = cv_results['train_score'].mean() - cv_results['test_score'].mean() \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1_x_bin, data[Target])\n",
    "\n",
    "    try:\n",
    "      Feature_Importance[MLA_name] = alg.feature_importances_\n",
    "    except AttributeError:\n",
    "      pass\n",
    "      \n",
    "    MLA_predict[MLA_name] = alg.predict(data1_x_bin)\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "MLA_compare['Difference'] = (MLA_compare['MLA Test Accuracy Mean']-MLA_compare['MLA Train Accuracy Mean'])*100\n",
    "MLA_compare\n",
    "\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA_compare.sort_values(by=\"Difference\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
