{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return metrics.roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols):    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for c in catcols:\n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil((num_unique_values)/2), 50))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(0.3)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    x = layers.Concatenate()(outputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/cat-in-the-dat-ii/train.csv\")\n",
    "test = pd.read_csv(\"../input/cat-in-the-dat-ii/test.csv\")\n",
    "sample = pd.read_csv(\"../input/cat-in-the-dat-ii/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"target\"] = -1\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
    "\n",
    "for feat in features:\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    data[feat] = lbl_enc.fit_transform(data[feat].fillna(\"-1\").astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test = data[data.target == -1].reset_index(drop=True)\n",
    "test_data = [test.loc[:, features].values[:, k] for k in range(test.loc[:, features].values.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 569999 samples, validate on 30001 samples\n",
      "Epoch 1/100\n",
      "569999/569999 [==============================] - 19s 34us/sample - loss: 0.4694 - auc: 0.6993 - val_loss: 0.4084 - val_auc: 0.7820\n",
      "Epoch 2/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4089 - auc: 0.7690 - val_loss: 0.3957 - val_auc: 0.7879\n",
      "Epoch 3/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4037 - auc: 0.7774 - val_loss: 0.3959 - val_auc: 0.7880\n",
      "Epoch 4/100\n",
      "569999/569999 [==============================] - 14s 25us/sample - loss: 0.4010 - auc: 0.7815 - val_loss: 0.3957 - val_auc: 0.7892\n",
      "Epoch 5/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3990 - auc: 0.7846 - val_loss: 0.3973 - val_auc: 0.7880\n",
      "Epoch 6/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3964 - auc: 0.7884 - val_loss: 0.3978 - val_auc: 0.7869\n",
      "Epoch 7/100\n",
      "569344/569999 [============================>.] - ETA: 0s - loss: 0.3939 - auc: 0.7921\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3939 - auc: 0.7920 - val_loss: 0.3995 - val_auc: 0.7852\n",
      "Epoch 8/100\n",
      "569999/569999 [==============================] - 14s 25us/sample - loss: 0.3858 - auc: 0.8037 - val_loss: 0.4027 - val_auc: 0.7809\n",
      "Epoch 9/100\n",
      "569344/569999 [============================>.] - ETA: 0s - loss: 0.3816 - auc: 0.8095Restoring model weights from the end of the best epoch.\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3816 - auc: 0.8095 - val_loss: 0.4078 - val_auc: 0.7786\n",
      "Epoch 00009: early stopping\n",
      "0.7892253519090667\n",
      "Train on 569999 samples, validate on 30001 samples\n",
      "Epoch 1/100\n",
      "569999/569999 [==============================] - 18s 31us/sample - loss: 0.4729 - auc: 0.6959 - val_loss: 0.4111 - val_auc: 0.7727\n",
      "Epoch 2/100\n",
      "569999/569999 [==============================] - 14s 25us/sample - loss: 0.4080 - auc: 0.7703 - val_loss: 0.4021 - val_auc: 0.7779\n",
      "Epoch 3/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4032 - auc: 0.7781 - val_loss: 0.4029 - val_auc: 0.7772\n",
      "Epoch 4/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4006 - auc: 0.7824 - val_loss: 0.4030 - val_auc: 0.7762\n",
      "Epoch 5/100\n",
      "568320/569999 [============================>.] - ETA: 0s - loss: 0.3985 - auc: 0.7853\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "569999/569999 [==============================] - 13s 22us/sample - loss: 0.3985 - auc: 0.7853 - val_loss: 0.4057 - val_auc: 0.7750\n",
      "Epoch 6/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3933 - auc: 0.7925 - val_loss: 0.4043 - val_auc: 0.7765\n",
      "Epoch 7/100\n",
      "569344/569999 [============================>.] - ETA: 0s - loss: 0.3910 - auc: 0.7961Restoring model weights from the end of the best epoch.\n",
      "569999/569999 [==============================] - 14s 24us/sample - loss: 0.3910 - auc: 0.7962 - val_loss: 0.4074 - val_auc: 0.7743\n",
      "Epoch 00007: early stopping\n",
      "0.7800037539537128\n",
      "Train on 569999 samples, validate on 30001 samples\n",
      "Epoch 1/100\n",
      "569999/569999 [==============================] - 18s 32us/sample - loss: 0.4735 - auc: 0.6956 - val_loss: 0.4099 - val_auc: 0.7679\n",
      "Epoch 2/100\n",
      "569999/569999 [==============================] - 14s 25us/sample - loss: 0.4092 - auc: 0.7686 - val_loss: 0.4001 - val_auc: 0.7729\n",
      "Epoch 3/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4033 - auc: 0.7781 - val_loss: 0.3998 - val_auc: 0.7725\n",
      "Epoch 4/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.4007 - auc: 0.7820 - val_loss: 0.3997 - val_auc: 0.7725\n",
      "Epoch 5/100\n",
      "568320/569999 [============================>.] - ETA: 0s - loss: 0.3987 - auc: 0.7853\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3986 - auc: 0.7853 - val_loss: 0.4007 - val_auc: 0.7718\n",
      "Epoch 6/100\n",
      "569999/569999 [==============================] - 13s 23us/sample - loss: 0.3937 - auc: 0.7921 - val_loss: 0.4021 - val_auc: 0.7704\n",
      "Epoch 7/100\n",
      "569344/569999 [============================>.] - ETA: 0s - loss: 0.3915 - auc: 0.7954Restoring model weights from the end of the best epoch.\n",
      "569999/569999 [==============================] - 14s 25us/sample - loss: 0.3915 - auc: 0.7954 - val_loss: 0.4049 - val_auc: 0.7680\n",
      "Epoch 00007: early stopping\n",
      "0.7825416883364478\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 18s 32us/sample - loss: 0.4713 - auc: 0.6980 - val_loss: 0.4076 - val_auc: 0.7719\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.4089 - auc: 0.7693 - val_loss: 0.3979 - val_auc: 0.7776\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.4035 - auc: 0.7779 - val_loss: 0.3998 - val_auc: 0.7781\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4005 - auc: 0.7821 - val_loss: 0.3985 - val_auc: 0.7762\n",
      "Epoch 5/100\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.3988 - auc: 0.7848 - val_loss: 0.3986 - val_auc: 0.7756\n",
      "Epoch 6/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7880\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3966 - auc: 0.7880 - val_loss: 0.3990 - val_auc: 0.7756\n",
      "Epoch 7/100\n",
      "567296/570000 [============================>.] - ETA: 0s - loss: 0.3904 - auc: 0.7970Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.3904 - auc: 0.7971 - val_loss: 0.4019 - val_auc: 0.7739\n",
      "Epoch 00007: early stopping\n",
      "0.7872164798761132\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 18s 32us/sample - loss: 0.4707 - auc: 0.6972 - val_loss: 0.4060 - val_auc: 0.7748\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.4087 - auc: 0.7694 - val_loss: 0.3966 - val_auc: 0.7787\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.4038 - auc: 0.7776 - val_loss: 0.3965 - val_auc: 0.7793\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4005 - auc: 0.7825 - val_loss: 0.3965 - val_auc: 0.7786\n",
      "Epoch 5/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3986 - auc: 0.7855 - val_loss: 0.3988 - val_auc: 0.7780\n",
      "Epoch 6/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3965 - auc: 0.7883\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3965 - auc: 0.7884 - val_loss: 0.3985 - val_auc: 0.7761\n",
      "Epoch 7/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3903 - auc: 0.7971Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.3903 - auc: 0.7972 - val_loss: 0.4021 - val_auc: 0.7748\n",
      "Epoch 00007: early stopping\n",
      "0.7881461716699475\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 18s 31us/sample - loss: 0.4734 - auc: 0.6951 - val_loss: 0.4091 - val_auc: 0.7673\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.4089 - auc: 0.7691 - val_loss: 0.3991 - val_auc: 0.7741\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.4036 - auc: 0.7777 - val_loss: 0.4006 - val_auc: 0.7745\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4008 - auc: 0.7823 - val_loss: 0.4010 - val_auc: 0.7740\n",
      "Epoch 5/100\n",
      "570000/570000 [==============================] - 13s 22us/sample - loss: 0.3984 - auc: 0.7859 - val_loss: 0.3994 - val_auc: 0.7734\n",
      "Epoch 6/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7882\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3966 - auc: 0.7882 - val_loss: 0.4020 - val_auc: 0.7718\n",
      "Epoch 7/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3904 - auc: 0.7969Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.3904 - auc: 0.7969 - val_loss: 0.4037 - val_auc: 0.7688\n",
      "Epoch 00007: early stopping\n",
      "0.7839697387210612\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 18s 32us/sample - loss: 0.4717 - auc: 0.6966 - val_loss: 0.4093 - val_auc: 0.7714\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.4086 - auc: 0.7697 - val_loss: 0.3987 - val_auc: 0.7762\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.4032 - auc: 0.7783 - val_loss: 0.4001 - val_auc: 0.7776\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4005 - auc: 0.7826 - val_loss: 0.4022 - val_auc: 0.7750\n",
      "Epoch 5/100\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.3988 - auc: 0.7850 - val_loss: 0.4013 - val_auc: 0.7758\n",
      "Epoch 6/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3960 - auc: 0.7891\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3959 - auc: 0.7891 - val_loss: 0.3997 - val_auc: 0.7737\n",
      "Epoch 7/100\n",
      "570000/570000 [==============================] - 15s 26us/sample - loss: 0.3902 - auc: 0.7972 - val_loss: 0.4022 - val_auc: 0.7719\n",
      "Epoch 8/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3872 - auc: 0.8015Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3872 - auc: 0.8015 - val_loss: 0.4040 - val_auc: 0.7692\n",
      "Epoch 00008: early stopping\n",
      "0.7871170754221628\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 19s 33us/sample - loss: 0.4713 - auc: 0.6970 - val_loss: 0.4146 - val_auc: 0.7718\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 15s 26us/sample - loss: 0.4084 - auc: 0.7699 - val_loss: 0.3991 - val_auc: 0.7761\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4030 - auc: 0.7789 - val_loss: 0.4021 - val_auc: 0.7753\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.4007 - auc: 0.7822 - val_loss: 0.3985 - val_auc: 0.7751\n",
      "Epoch 5/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3988 - auc: 0.7849\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 13s 24us/sample - loss: 0.3988 - auc: 0.7849 - val_loss: 0.4026 - val_auc: 0.7729\n",
      "Epoch 6/100\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.3933 - auc: 0.7927 - val_loss: 0.4027 - val_auc: 0.7714\n",
      "Epoch 7/100\n",
      "568320/570000 [============================>.] - ETA: 0s - loss: 0.3906 - auc: 0.7967Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3906 - auc: 0.7966 - val_loss: 0.4039 - val_auc: 0.7702\n",
      "Epoch 00007: early stopping\n",
      "0.7856525639331475\n",
      "Train on 570000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "570000/570000 [==============================] - 19s 33us/sample - loss: 0.4719 - auc: 0.6957 - val_loss: 0.4099 - val_auc: 0.7618\n",
      "Epoch 2/100\n",
      "570000/570000 [==============================] - 15s 25us/sample - loss: 0.4089 - auc: 0.7694 - val_loss: 0.3989 - val_auc: 0.7653\n",
      "Epoch 3/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.4032 - auc: 0.7783 - val_loss: 0.3993 - val_auc: 0.7650\n",
      "Epoch 4/100\n",
      "570000/570000 [==============================] - 15s 26us/sample - loss: 0.4007 - auc: 0.7823 - val_loss: 0.3978 - val_auc: 0.7764\n",
      "Epoch 5/100\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.3988 - auc: 0.7849 - val_loss: 0.4014 - val_auc: 0.7736\n",
      "Epoch 6/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7883\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "570000/570000 [==============================] - 14s 24us/sample - loss: 0.3966 - auc: 0.7883 - val_loss: 0.4007 - val_auc: 0.7737\n",
      "Epoch 7/100\n",
      "570000/570000 [==============================] - 13s 23us/sample - loss: 0.3902 - auc: 0.7971 - val_loss: 0.4038 - val_auc: 0.7690\n",
      "Epoch 8/100\n",
      "569344/570000 [============================>.] - ETA: 0s - loss: 0.3861 - auc: 0.8032Restoring model weights from the end of the best epoch.\n",
      "570000/570000 [==============================] - 14s 25us/sample - loss: 0.3861 - auc: 0.8032 - val_loss: 0.4060 - val_auc: 0.7669\n",
      "Epoch 00008: early stopping\n",
      "0.7853257761266087\n",
      "Train on 570001 samples, validate on 29999 samples\n",
      "Epoch 1/100\n",
      "570001/570001 [==============================] - 18s 32us/sample - loss: 0.4724 - auc: 0.6949 - val_loss: 0.4100 - val_auc: 0.7724\n",
      "Epoch 2/100\n",
      "570001/570001 [==============================] - 13s 23us/sample - loss: 0.4088 - auc: 0.7691 - val_loss: 0.3979 - val_auc: 0.7776\n",
      "Epoch 3/100\n",
      "570001/570001 [==============================] - 14s 25us/sample - loss: 0.4031 - auc: 0.7785 - val_loss: 0.3966 - val_auc: 0.7787\n",
      "Epoch 4/100\n",
      "570001/570001 [==============================] - 13s 23us/sample - loss: 0.4011 - auc: 0.7816 - val_loss: 0.3992 - val_auc: 0.7787\n",
      "Epoch 5/100\n",
      "570001/570001 [==============================] - 13s 23us/sample - loss: 0.3987 - auc: 0.7851 - val_loss: 0.3973 - val_auc: 0.7780\n",
      "Epoch 6/100\n",
      "352256/570001 [=================>............] - ETA: 4s - loss: 0.3966 - auc: 0.7898"
     ]
    }
   ],
   "source": [
    "oof_preds = np.zeros((len(train)))\n",
    "test_preds = np.zeros((len(test)))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "for train_index, test_index in skf.split(train, train.target.values):\n",
    "    X_train, X_test = train.iloc[train_index, :], train.iloc[test_index, :]\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train, y_test = X_train.target.values, X_test.target.values\n",
    "    model = create_model(data, features)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "    X_train = [X_train.loc[:, features].values[:, k] for k in range(X_train.loc[:, features].values.shape[1])]\n",
    "    X_test = [X_test.loc[:, features].values[:, k] for k in range(X_test.loc[:, features].values.shape[1])]\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=5,\n",
    "                                 verbose=1, mode='max', baseline=None, restore_best_weights=True)\n",
    "\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n",
    "                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              utils.to_categorical(y_train),\n",
    "              validation_data=(X_test, utils.to_categorical(y_test)),\n",
    "              verbose=1,\n",
    "              batch_size=1024,\n",
    "              callbacks=[es, rlr],\n",
    "              epochs=100\n",
    "             )\n",
    "    valid_fold_preds = model.predict(X_test)[:, 1]\n",
    "    test_fold_preds = model.predict(test_data)[:, 1]\n",
    "    oof_preds[test_index] = valid_fold_preds.ravel()\n",
    "    test_preds += test_fold_preds.ravel()\n",
    "    print(metrics.roc_auc_score(y_test, valid_fold_preds))\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC=0.7854754548417212\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall AUC={}\".format(metrics.roc_auc_score(train.target.values, oof_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission file\n"
     ]
    }
   ],
   "source": [
    "test_preds /= 20\n",
    "test_ids = test.id.values\n",
    "print(\"Saving submission file\")\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_ids,\n",
    "    'target': test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
